@book{pooleArtificialIntelligenceFoundations2017,
  title = {Artificial Intelligence: Foundations of Computational Agents},
  shorttitle = {Artificial Intelligence},
  author = {Poole, David Lynton and Mackworth, Alan K.},
  date = {2017},
  edition = {2nd ed},
  publisher = {{Cambridge university press}},
  location = {{Cambridge, UK}},
  isbn = {978-1-107-19539-4},
  langid = {english}
} 

@online{kumarProfessionalsPointAdvantages2019,
  title = {The {{Professionals Point}}: {{Advantages}} and {{Disadvantages}} of {{Linear Regression}} in {{Machine Learning}}},
  shorttitle = {The {{Professionals Point}}},
  author = {Kumar, Naresh},
  date = {2019-05-09},
  url = {http://theprofessionalspoint.blogspot.com/2019/05/advantages-and-disadvantages-of-linear},
  urldate = {2022-04-25},
  organization = {{The Professionals Point}},
  keywords = {Algorithms,Machine Learning}
}

@book{hands-onML,
  title = {Hands-on Machine Learning with Scikit-Learn, Keras \& TensorFlow},
  shorttitle = {Concepts, Tools, and Techniques to Build Intelligent Systems},
  author = {Aurélien Géron},
  date = {2019},
  edition = {2nd ed},
  publisher = {{O'Riley Media Inc}},
  location = {{Sebastapol, US}},
  isbn = {9781492032649},
  langid = {english}
} 

@book{AIModernApproach,
  title = {Artificial Intelligence A Modern Approach},
  author = {Peter Norvig and Stuart Russel},
  date = {2021},
  edition = {4th ed},
  publisher = {{Pearson Education Limited}},
  location = {{New York City, US}},
  isbn = {9781292401133},
  langid = {english}
} 

@inproceedings{AttentionIsAllYouNeed,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@Article{LSTMPaper,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@inproceedings{RNNPaper,
    title = {Learning Phrase Representations using RNN Encoder Decoder for Statistical Machine Translation},
    author = {Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Fethi Bougares and Holger Schwenk and Yoshua Bengio and Dzmitry Bahdanau},
    year = {2014}
}

@inproceedings{HybridArimaAndNN,
 author = {G. Peter Zhang et. al.},
 pages = {},
 publisher = {Elsevier Science B.V.},
 title = {Time series forecasting using a hybrid ARIMA and neural network model},
 volume = {},
 year = {2001}
}

@book{ForecastinPrinciplesAndPractice,
  title = {Forecasting: Principles and Practice},
  author = {Rob J. Hyndman, George Athanasopoulos},
  date = {2021},
  edition = {2nd ed},
  publisher = {{OTexts}},
  location = {{Melbourne, Australia}},
  isbn = {978-0987507112},
  langid = {english}
}

@inproceedings{cirsteaEnhanceNetPluginNeural2021,
  title = {{{EnhanceNet}}: {{Plugin Neural Networks}} for {{Enhancing Correlated Time Series Forecasting}}},
  shorttitle = {{{EnhanceNet}}},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Cirstea, Razvan-Gabriel and Kieu, Tung and Guo, Chenjuan and Yang, Bin and Pan, Sinno Jialin},
  date = {2021-04},
  pages = {1739--1750},
  issn = {2375-026X},
  doi = {10.1109/ICDE51399.2021.00153},
  abstract = {Correlated time series forecasting plays an essential role in many cyber-physical systems, where entities interact with each other over time. To enable accurate forecasting, it is essential to capture both the temporal dynamics and the correlations among different entities. To capture the former, two popular types of models, recurrent neural networks (RNNs) and temporal convolution networks (TCNs), are employed. To capture the latter, a graph is constructed to reflect certain relationships among entities and then graph convolution (GC) is applied upon the graph to capture the correlations among the entities. The state-of-the-art forecasting accuracy is achieved by models that combine RNNs or TCNs with GC. However, they neither capture distinct temporal dynamics that exist among different entities nor consider the entity correlations that evolve across time. In this paper, rather than proposing yet another new end-to-end forecasting model, we aim at providing a framework to enhance existing forecasting models, where we propose generic plugins that can be easily integrated into existing solutions to solve the two challenges and thus further enhance their accuracy. Specifically, we propose two plugin neural networks that are able to better capture distinct temporal dynamics for different entities and dynamic entity correlations across time, so that forecasting accuracy is improved while model parameters to be learned are reduced. Experimental results on three real-world correlated time series data sets demonstrate that the proposed framework with the two plugin networks is able to achieve the above goals.},
  eventtitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Conferences,Convolution,Correlation,Cyber-physical systems,dynamic weights,neural networks,Predictive models,Recurrent neural networks,Time series analysis,time series forecasting}
}


@online{bentoMultilayerPerceptronExplained2021,
  title = {Multilayer {{Perceptron Explained}} with a {{Real-Life Example}} and {{Python Code}}: {{Sentiment Analysis}}},
  shorttitle = {Multilayer {{Perceptron Explained}} with a {{Real-Life Example}} and {{Python Code}}},
  author = {Bento, Carolina},
  date = {2021-09-30T04:15:17},
  url = {https://towardsdatascience.com/multilayer-perceptron-explained-with-a-real-life-example-and-python-code-sentiment-analysis-cb408ee93141},
  urldate = {2022-05-04},
  abstract = {Multilayer Perceptron is a Neural Network that learns the relationship between linear and non-linear data.},
  langid = {english},
}