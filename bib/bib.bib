@book{pooleArtificialIntelligenceFoundations2017,
  title = {Artificial Intelligence: Foundations of Computational Agents},
  shorttitle = {Artificial Intelligence},
  author = {Poole, David Lynton and Mackworth, Alan K.},
  date = {2017},
  edition = {2nd ed},
  publisher = {{Cambridge university press}},
  location = {{Cambridge, UK}},
  isbn = {978-1-107-19539-4},
  langid = {english}
} 

@online{kumarProfessionalsPointAdvantages2019,
  title = {The {{Professionals Point}}: {{Advantages}} and {{Disadvantages}} of {{Linear Regression}} in {{Machine Learning}}},
  shorttitle = {The {{Professionals Point}}},
  author = {Kumar, Naresh},
  date = {2019-05-09},
  url = {http://theprofessionalspoint.blogspot.com/2019/05/advantages-and-disadvantages-of-linear},
  urldate = {2022-04-25},
  organization = {{The Professionals Point}},
  keywords = {Algorithms,Machine Learning}
}

@book{hands-onML,
  title = {Hands-on Machine Learning with Scikit-Learn, Keras \& TensorFlow},
  shorttitle = {Concepts, Tools, and Techniques to Build Intelligent Systems},
  author = {Aurélien Géron},
  date = {2019},
  edition = {2nd ed},
  publisher = {{O'Riley Media Inc}},
  location = {{Sebastapol, US}},
  isbn = {9781492032649},
  langid = {english}
} 

@book{AIModernApproach,
  title = {Artificial Intelligence A Modern Approach},
  author = {Peter Norvig and Stuart Russel},
  date = {2021},
  edition = {4th ed},
  publisher = {{Pearson Education Limited}},
  location = {{New York City, US}},
  isbn = {9781292401133},
  langid = {english}
} 

@inproceedings{AttentionIsAllYouNeed,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/
 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@Article{LSTMPaper,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@inproceedings{RNNPaper,
    title = {Learning Phrase Representations using RNN Encoder Decoder for Statistical Machine Translation},
    author = {Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Fethi Bougares and Holger Schwenk and Yoshua Bengio and Dzmitry Bahdanau},
    year = {2014}
}

@inproceedings{HybridArimaAndNN,
 author = {G. Peter Zhang et. al.},
 pages = {},
 publisher = {Elsevier Science B.V.},
 title = {Time series forecasting using a hybrid ARIMA and neural network model},
 volume = {},
 year = {2001}
}

@book{ForecastinPrinciplesAndPractice,
  title = {Forecasting: Principles and Practice},
  author = {Rob J. Hyndman, George Athanasopoulos},
  date = {2021},
  edition = {2nd ed},
  publisher = {{OTexts}},
  location = {{Melbourne, Australia}},
  isbn = {978-0987507112},
  langid = {english}
}

@inproceedings{cirsteaEnhanceNetPluginNeural2021,
  title = {{{EnhanceNet}}: {{Plugin Neural Networks}} for {{Enhancing Correlated Time Series Forecasting}}},
  shorttitle = {{{EnhanceNet}}},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Cirstea, Razvan-Gabriel and Kieu, Tung and Guo, Chenjuan and Yang, Bin and Pan, Sinno Jialin},
  date = {2021-04},
  pages = {1739--1750},
  issn = {2375-026X},
  doi = {10.1109/ICDE51399.2021.00153},
  abstract = {Correlated time series forecasting plays an essential role in many cyber-physical systems, where entities interact with each other over time. To enable accurate forecasting, it is essential to capture both the temporal dynamics and the correlations among different entities. To capture the former, two popular types of models, recurrent neural networks (RNNs) and temporal convolution networks (TCNs), are employed. To capture the latter, a graph is constructed to reflect certain relationships among entities and then graph convolution (GC) is applied upon the graph to capture the correlations among the entities. The state-of-the-art forecasting accuracy is achieved by models that combine RNNs or TCNs with GC. However, they neither capture distinct temporal dynamics that exist among different entities nor consider the entity correlations that evolve across time. In this paper, rather than proposing yet another new end-to-end forecasting model, we aim at providing a framework to enhance existing forecasting models, where we propose generic plugins that can be easily integrated into existing solutions to solve the two challenges and thus further enhance their accuracy. Specifically, we propose two plugin neural networks that are able to better capture distinct temporal dynamics for different entities and dynamic entity correlations across time, so that forecasting accuracy is improved while model parameters to be learned are reduced. Experimental results on three real-world correlated time series data sets demonstrate that the proposed framework with the two plugin networks is able to achieve the above goals.},
  eventtitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Conferences,Convolution,Correlation,Cyber-physical systems,dynamic weights,neural networks,Predictive models,Recurrent neural networks,Time series analysis,time series forecasting}
}

@online{bentoMultilayerPerceptronExplained2021,
  title = {Multilayer {{Perceptron Explained}} with a {{Real-Life Example}} and {{Python Code}}: {{Sentiment Analysis}}},
  shorttitle = {Multilayer {{Perceptron Explained}} with a {{Real-Life Example}} and {{Python Code}}},
  author = {Bento, Carolina},
  date = {2021-09-30T04:15:17},
  url = {https://towardsdatascience.com/multilayer-perceptron-explained-with-a-real-life-example-and-python-code-sentiment-analysis-cb408ee93141},
  urldate = {2022-05-04},
  abstract = {Multilayer Perceptron is a Neural Network that learns the relationship between linear and non-linear data.},
  langid = {english},
}

@online{TransformersScratchPeterbloem,
  title = {Transformers from Scratch},
  url = {http://peterbloem.nl/blog/transformers},
  urldate = {2022-05-04},
  author={Bloem, Peter},
}

@article{time2vec,
	title = {Time2Vec: Learning a Vector Representation of Time},
	url = {http://arxiv.org/abs/1907.05321},
	shorttitle = {Time2Vec},
	abstract = {Time is an important feature in many applications involving events that occur synchronously and/or asynchronously. To effectively consume time information, recent studies have focused on designing new architectures. In this paper, we take an orthogonal but complementary approach by providing a model-agnostic vector representation for time, called Time2Vec, that can be imported into many existing and future architectures and improve their performances. We show on a range of models and problems that replacing the notion of time with its Time2Vec representation improves the performance of the final model.},
	journaltitle = {{arXiv}:1907.05321 [cs]},
	author = {Kazemi, Seyed Mehran and Goel, Rishab and Eghbali, Sepehr and Ramanan, Janahan and Sahota, Jaspreet and Thakur, Sanjay and Wu, Stella and Smyth, Cathal and Poupart, Pascal and Brubaker, Marcus},
	urldate = {2022-05-09},
	date = {2019-07-11},
	eprinttype = {arxiv},
	eprint = {1907.05321},
	keywords = {Computer Science - Machine Learning},
}

@online{schmitz_stock_2020,
	title = {Stock predictions with state-of-the-art Transformer and Time Embeddings},
	url = {https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6},
	abstract = {Feeding a trading bot with state-of-the-art stock predictions},
	titleaddon = {Medium},
	author = {Schmitz, Jan},
	urldate = {2022-05-11},
	date = {2020-09-16},
	langid = {english},
}

@online{deepnote,
    title = {Deepnote},
    url = {https://deepnote.com},
    abstract = {Managed notebooks for data scientists and researchers.},
    titleaddon = {Deepnote},
    urldate = {2022-05-11},
    langid = {english},
}

@online{kaggle,
	title = {Kaggle: Your Machine Learning and Data Science Community},
	url = {https://www.kaggle.com/},
	shorttitle = {Kaggle},
	abstract = {Kaggle is the world's largest data science community with powerful tools and resources to help you achieve your data science goals.},
	urldate = {2022-05-17},
	langid = {english},
}

@online{pickle_documentation,
	title = {pickle — Python object serialization — Python 3.10.4 documentation},
	url = {https://docs.python.org/3/library/pickle.html},
	urldate = {2022-05-18},
}

@online{streamlit,
	title = {Streamlit • The fastest way to build and share data apps},
	url = {https://streamlit.io/},
	abstract = {Streamlit is an open-source app framework for Machine Learning and Data Science teams. Create beautiful web apps in minutes.},
	urldate = {2022-05-18},
	langid = {english},
}

@online{pytorch,
	title = {{PyTorch}},
	url = {https://www.pytorch.org},
	abstract = {An open source machine learning framework that accelerates the path from research prototyping to production deployment.},
	urldate = {2022-05-18},
	langid = {english},
}

@online{scikit-learn,
	title = {scikit-learn: machine learning in Python — scikit-learn 1.1.0 documentation},
	url = {https://scikit-learn.org/stable/},
	urldate = {2022-05-18},
}

@online{tensorflow,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	urldate = {2022-05-18},
}

@misc{GRU-paper,
  doi = {10.48550/ARXIV.1409.1259},
  url = {https://arxiv.org/abs/1409.1259},
  author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  keywords = {Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
  publisher = {arXiv},
  year = {2014},
}

@article{RNN-paper,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

@techreport{MLP-paper,
  title={Principles of neurodynamics. perceptrons and the theory of brain mechanisms},
  author={Rosenblatt, Frank},
  year={1961},
  institution={Cornell Aeronautical Lab Inc Buffalo NY}
}

@book{ARIMA-book,
  title={Time series analysis: forecasting and control},
  author={Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year={2015},
  publisher={John Wiley \& Sons}
}

@online{WeatherArchiveJena,
  title = {Weather Archive {{Jena}}},
  url = {https://www.kaggle.com/pankrzysiu/weather-archive-jena},
  urldate = {2022-05-23},
  abstract = {Air temperature, atmospheric pressure, humidity, etc recorded over seven years},
  langid = {english}
}

@online{HistoricalHourlyWeather,
  title = {Historical {{Hourly Weather Data}} 2012-2017},
  url = {https://www.kaggle.com/selfishgene/historical-hourly-weather-data},
  urldate = {2022-05-23},
  abstract = {Hourly weather data for 30 US \& Canadian Cities + 6 Israeli Cities},
  langid = {english}
}

@online{BangladeshWeatherDataset,
  title = {Bangladesh {{Weather Dataset}}},
  url = {https://www.kaggle.com/yakinrubaiat/bangladesh-weather-dataset},
  urldate = {2022-05-23},
  abstract = {Weather Data from 1901 to 2015},
  langid = {english}
}

@online{WeatherSzeged,
  title = {Weather in {{Szeged}} 2006-2016},
  url = {https://www.kaggle.com/budincsevity/szeged-weather},
  urldate = {2022-05-23},
  abstract = {Hourly/daily summary with temperature, pressure, wind speed and more},
  langid = {english}
}

@online{DailyClimateTime,
  title = {Daily {{Climate}} Time Series Data | {{Kaggle}}},
  url = {https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data},
  urldate = {2022-05-23}
}

@book{alurPrinciplesCyberphysicalSystems2015,
  title = {Principles of Cyber-Physical Systems},
  author = {Alur, Rajeev},
  date = {2015},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-02911-7},
  pagetotal = {446},
  keywords = {Automatic control,Embedded internet devices,Formal methods (Computer science),Internet of things,System design}
}