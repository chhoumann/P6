https://www.investopedia.com/terms/a/autoregressive-integrated-moving-average-arima.asp
https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.462.3756&rep=rep1&type=pdf

Auto regressive integrated moving average (ARIMA) is a linear statistical analysis model that uses time series data to predict future trends. It comprises two models; The Auto-regression (AR) model which is a model that forecasts a variable using a linear combination of past values of the same variable. The moving average model (MA), which instead of using past values, uses previously forecasted errors (residuals) to make current predictions. 
The final element in the ARIMA model, the I is the differencing of observations in order to achieve stationarity, which is done to remove trends or seasonality, which will affect the value of the time series at different times. The primary drawback of the ARIMA model is the assumption of linearity in the time series data, which may in fact be non-linear. (https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.462.3756&rep=rep1&type=pdf)
(https://otexts.com/fpp2/MA.html)

// this is just an idea
Another approach to time series forecasting is the use of neural networks. A popular model is to use recurrent neural networks and derivations of this model. Recurrent neural networks are similar to feedforward networks, but it has a connection pointing backwards and can be used to generate sequences, or to label or classify sequences. The backwards connection acts as a memory in the network, as it preserves some of the state across time, which makes it capable of learning short patterns. A primary issue with RNN’s are the vanishing gradient and the exploding gradient problem, which more powerful models seek to improve upon. (Artificial intelligence: A modern approach, 4th edition)(Hands on Machine learning with scikit-learn, Keras and TensorFlow, 2nd edition)

To combat the issues of the vanishing gradient and exploding gradient problem, the Long short-term memory (LSTM) model was introduced by Sepp Hochreiter et. al. The basic principle of this model is the ability to bridge time intervals across longer time steps, which is achieved through the maintenance, updating and filtering of information into a cell state, which propagates through the entire network (LONG SHORT-TERM MEMORY, 1997, Sepp Hochreiter et. al.). A more recent, simplified version of the LSTM model, is the gated recurrent network (GRU), proposed by Kyunghyun Cho et al. which merges the two state vectors in the LSTM into a single state vector, includes just a single gate controller that controls both the forget gate and the input gate and instead of an output gate which is present in the LSTM, the GRU outputs the entire state vector at every time step. (Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation)(Hands on Machine learning with scikit-learn, Keras and TensorFlow, 2nd edition).
Although LSTM and GRU models show significant improvements over regular RNN models in their ability to learn long term patterns, they are still inefficient at learning longer patterns as their inherent sequential nature can cause memory constraints. (attention is all you need, Ashish Vaswani et. al.)
