\subsection{ARIMA models}
Auto regressive integrated moving average (ARIMA) is a linear statistical analysis model that uses time series data to predict future trends. It comprises two models; The Auto-regression (AR) model which is a model that forecasts a variable using a linear combination of past values of the same variable. The moving average model (MA), which instead of using past values, uses previously forecasted errors (residuals) to make current predictions. 
The final element in the ARIMA model, the I, is the differencing of observations in order to achieve stationarity, which is done to remove trends or seasonality, which will affect the value of the time series at different times. The primary drawback of the ARIMA model is the assumption of linearity in the time series data, which may in fact be non-linear.\cite{HybridArimaAndNN}\cite{ForecastinPrinciplesAndPractice}

\subsection{RNN models}
Another approach to time series forecasting is the use of neural networks. A popular model is to use recurrent neural networks and derivations of this model. Recurrent neural networks are similar to feedforward networks, but have memory of the past which is achieved through a hidden state which is passed reccurently through from previous layer as input to the successing layer. This makes it capable of learning short patterns. 
A primary issue with RNNâ€™s are the vanishing gradient and the exploding gradient problem with longer patterns, which more powerful models seek to improve upon. \cite{AIModernApproach}\cite{hands-onML}

\subsection{LSTM and GRU models}
To combat the issues of the vanishing gradient and exploding gradient problem, the Long short-term memory (LSTM) model was introduced by Sepp Hochreiter et. al. The basic principle of this model is the ability to bridge time intervals across longer time steps, which is achieved through the maintenance, updating and filtering of information into a cell state, which propagates through the entire network \cite{LSTMPaper}. 
A more recent, simplified version of the LSTM model, is the gated recurrent network (GRU), proposed by Kyunghyun Cho et al. which merges the two state vectors in the LSTM into a single state vector, includes just a single gate controller that controls both the forget gate and the input gate and instead of an output gate which is present in the LSTM, the GRU outputs the entire state vector at every time step\cite{RNNPaper} \cite{hands-onML}.
Although LSTM and GRU models show significant improvements over regular RNN models in their ability to learn long term patterns, they are still inefficient at learning longer patterns as their inherent sequential nature can cause memory constraints\cite{AttentionIsAllYouNeed}. 
