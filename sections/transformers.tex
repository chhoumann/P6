\subsection{Which problems transformers solve}
The point of transformers was to overcome the problems that previous state of the art architectures have faced. Transformers aim to get the best features from the RNN and Convolutional Neural Network (CNN) models.

The RNN model has two notable weaknesses. First, its inability to learn long term patterns, due to the exploding/vanishing gradient problems that occur during backpropagation.
Second, its recurrent connection is also a weakness. This is because, as information is propagated along a sequence, you cannot compute the cell at time step $i$ until you've computed the cell at time step $i-1$.

However, this is one of the benefits of CNNs---they can be computed concurrently. However, they are unable to learn even short term patterns as well. They can only learn patterns that fit within their kernel size.

Transformers try to get the best of both.
They can model dependencies over the whole range of the input sequence as easily it can model those next to each other. And there are no recurrent connections, so it can be computed very efficiently using parallel computation. This is facilitated through the use of the self-attention mechanism.\cite{TransformersScratchPeterbloem}


\subsection{Self-Attention Mechanism}
This is a sequence to sequence operation. What this means is that input is a sequence of vectors $x_{1},x_{2},\ldots, x_{t}$, and output is also a sequence of vectors $y_{1},y_{2},\ldots, y_{t}$.
These vectors all have dimension $k$.

To get the output vector $y_{i}$, the operation takes a weighed average over all input vectors.
$$
y_{i}=\sum_{j}w_{ij}x_{j}
$$
Here, $j$ indexes over the whole sequence and the weights sum to one over all $j$.
The weight $w_{ij}$ is derived from a function over $x_{i}$ and $x_{j}$.
This could be the dot product:
$$
w'_{ij}=x_{i}^Tx_{j}
$$
where $x_{i}$ is the input vector at the same position as the current output vector $y_{i}$.
The next output vector has a completely different series of dot products, and therefore a different weighted sum.

As the dot product can give us anything between negative and positive infinity, the softmax activation function is used to map values to $[0,1]$, and to ensure that they sum to $1$ over the entire sequence:
$$
w_{ij}=\frac{\exp w'_{ij}}{\sum_{j}\exp w'_{ij}}
$$
For maximum efficiency, we vectorize as much as possible. So the calculations become:
\begin{gather}
W'=X^TX\\
W=\text{softmax}(W')\\
Y^T=WX^T
\end{gather}\cite{TransformersScratchPeterbloem}


\subsection{How transformers work}
Transformers, as mentioned, primarily use self-attention. This forms the basis for the architecture.

Transformer blocks have variations, so the one presented here isn't the end-all-be-all of the transformer.

First, the transformer block applies self-attention. Then it does layer normalization, after which it applies a feed forward network. The feed forward network here is a single MLP applied independently to each vector. And lastly, another layer normalization is applied.

Your network may become permutation invariant, meaning that no matter which order your words are in, you get the same output given the same words. This can be detrimental to the training process, as it means that the model is not learning the dependencies between words. Positions matter. If every word in this paper was ordered alphabetically, it wouldn't make much sense.

To solve this, one can use position embeddings or position encodings.

With position embedding, you embed the position of the word in the sentence. For this to work during training, you would need to see sequences of every length, otherwise the relevant position embeddings do not get trained.

Position encodings work in a similar way, except you do not learn the position vectors, you choose a function $f:\mathbb{N}\rightarrow\mathbb{R}^k$ that map the positions to real valued vectors. The network is left to figure out how to interpret these.\cite{TransformersScratchPeterbloem}