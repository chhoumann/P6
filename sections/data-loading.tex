As the purpose of the project is to create a system in which a user should be able to see the weather forecast for a specific area, or more precisely, the temperature forecast, we have developed a system that leverages several models to perform the temperature forecasting. 
The goal is to allow the user to interact with the system in such a way that the user can see the accuracy of the different models. 
The user is able to select a city for which they want to predict the temperature. The results are shown both as a graph that displays the predicted value versus the actual value, as well as the specific set of temperatures of that region for a given time period into the future. This is similar to what one may see on regular weather forecasts.

In order to achieve, this we have constructed a pipeline that preprocesses the data, trains our models, and the resulting models are then used to perform the forecasting displayed on a web application. 
Figure xx shows an overview of the architecture of the pipeline.
Note that since the transformer model is the best performing model, which is shown in section (experiment afsnit), this is the model that we chose to include and describe as part of the pipeline. Regardless of this fact, the other models are used in the pipeline in a similar fashion.

\subsection{Data loading and preprocessing}
In order to load the data, we use the Pandas library for Python.
Data is loaded from CSV files and then passed to the preprocessing stage.

Preprocessing is done by first stripping the Pandas DataFrame of all columns except the target column $y$. In our case, this is the column denoting the temperature.

Next, we add $n$ time lag features. Say $x_{t,y}$ denotes value of the target attribute $y$ at time step $t$ for $N$ sensors. Time lag features are added by, for each $x_{t}$, adding $L=\{1,\dots, n\}$ attributes where $L_{i=1}^n=x_{t-i, y}$.

As we used different Python frameworks for different models, the processing from here depended on the framework. Some required the dataset to be converted to tensors, and others did not.
Besides this, we also split the data into test and training sets with a $0.33/0.77$ distribution for test and training, respectively.

