\section*{Summary}
In this project, we are interested in doing time series forecasting using various machine and deep learning models.
The goal is to compare their performance, both on their accuracy as well as their computation time when training on different datasets.
Time series forecasting concerns itself with predicting future behavior of some phenomenon based on existing historical data.
In our case, this would be predicting future temperatures based on existing historical data from different regions in the world.
We aim to create an application that allows a user to select a trained model for use in predicting the temperature of a  region at some specified timestamp.

\subsection{Why Machine Learning}
Historically, weather forecasting has always been an important part of day-to-day life, and recently machine and deep learning has gradually become a more common technique for this.
Popular methods include the application of models such as ARIMA, RNNs, LSTMs and GRUs.
Each of these models have limitations that make them problematic to use in time series forecasting, however.
For models such as RNNs, the vanishing gradient problem limits its ability to learn over long time steps as the change of the internal weights will become miniscule, which, for long time steps, effectively prevents further training.
LSTMs and GRUs are improvements on RNN models, however they suffer from other issues such as the abundance of parameters that often lead to complex models that take a long time to train.
The primary model described and used in this project is the transformer model.
The transformer model has its origins in natural language processing and was introduced in 2017 by \citet{AttentionIsAllYouNeed}. 
The aim of this model was to overcome some of the issues related to both the vanishing and exploding gradient problem as well as the performance issues seen in some models.
The primary component of this model is an attention mechanism called multi-head attention, which allows the model to do training in parallel and thus improve performance. 
The model is originally also comprised of an encoder layer and a decoder layer. 

\subsection{Our work}
In the paper, we describe a selection of models in detail and compare the performance of these.
Furthermore, we also detail the changes and additions we made to the original architecture such that it can be used for time series forecasting.
Since the transformer model is indifferent to temporal information, we had to convert our input data into some vector representation containing both periodic and non-periodic information.
This was done following the approach presented in \citet{time2vec}.
In addition to this, we also removed the decoder layer, since we do not to output any decoded data.

% TODO: Describe the pipeline/architecture here
% - our product/innovation - the application/pipeline 
% 	- results overall - transformer performance vs the other models

\subsection{Collaboration}
We used Deepnote\cite{deepnote} to train the models. 
This was done for several different reasons; firstly, to achieve similar hardware performance on the training of all the models we used in the project.
The code for each model was containing in individual Python notebooks on Deepnote.
This allowed for easy collaboration between the group since Deepnote features a collaborative text editor.

The data we used are open source and downloaded from Kaggle\cite{kaggle}, and was provided by a variety of different contributors.
As such, it is worth noting that the data therefore came from arbitrary regions of the world, and that we are unable to verity its legitimacy.
This should be of no concern, however, since the primary goal was to use meaningfully large and representative datasets for training the models.
Finally, the models were trained using arbitrary settings for the batch sizes, hyperparameters, learning and dropout rates.


\subsection{Conclusion/ending}
% TODO: Conclusion goes here