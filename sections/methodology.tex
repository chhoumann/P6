\section{Methodology}
In order to train the models, we used Deepnote\cite{deepnote} to achieve similar hardware performance on the training of all the models used for this project. 
The training was done using individual Python notebooks contained in a project on Deepnote, in order to allow easy collaboration between the authors.
The data used are open source and was collected on Kaggle. It was provided by individual contributors for free use.
The data was therefore came from arbitrary regions of the world. 
It should be noted that we cannot verify the legitimacy of the data, the main objective was simply to gather datasets that were representative and meaningfully large to train the models on. 
The training of each model used arbitrary settings for the batch sizes, hyperparameters, learning rates and dropout rates. Similarly, no optimization algorithms were implemented in order to identify the optimal settings. 