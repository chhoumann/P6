\begin{table*}[t]
    \centering
    \caption {Summary of gathered results---the best results are highlighted in bold and underlined.} \label{tab:ResultsTable} 
    \begin{tabular}{|l|r|r|r|r|r|}
    \hline
    \multicolumn{1}{|c|}{Models} & \multicolumn{1}{c|}{Austin} & \multicolumn{1}{c|}{Bangladesh} & \multicolumn{1}{c|}{Delhi} & \multicolumn{1}{c|}{Jena} & \multicolumn{1}{c|}{Szeged} \\ \hline
    ARIMA                        & 14.9971                     & 4.1595                          & 7.4077                     & 8.4354                    & 9.5993                      \\ \hline
    GRU                          & 12.1855                     & 3.6176                          & 3.8810                     & 3.5758                    & 5.0462                      \\ \hline
    LinReg                       & 13.465                      & 3.6610                          & 7.8551                     & 8.4248                    & 9.5464                      \\ \hline
    LSTM                         & 15.5649                     & 3.6178                          & 4.2537                     & 3.6141                    & 5.1415                      \\ \hline
    MLP                          & 15.3179                     & 3.7288                          & 7.5508                     & 8.6366                    & 10.6112                     \\ \hline
    RNN                          & 11.2289                     & 3.6152                          & 6.8748                     & 3.0483                    & 4.8869                      \\ \hline
    Transformer                  & {\ul{\textbf{7.4627}}}      & {\ul{\textbf{1.8536}}}          & {\ul{\textbf{2.4665}}}     & {\ul{\textbf{1.1787}}}    & {\ul{\textbf{2.0066}}}      \\ \hline
    \end{tabular}
\end{table*}

\begin{table*}[t]
    \centering
    \caption {Results sorted by lowest summed error in ascending order for each dataset.} \label{tab:ResultsTableSummed} 
    \begin{tabular}{|l|r|r|r|r|r|r|}
    \hline
        \multicolumn{1}{|c|}{Models} & \multicolumn{1}{c|}{Austin} & \multicolumn{1}{c|}{Bangladesh} & \multicolumn{1}{c|}{Delhi} & \multicolumn{1}{c|}{Jena} & \multicolumn{1}{c|}{Szeged} & \multicolumn{1}{c|}{Sum} \\ \hline
        Transformer & 7.4627 & 1.8536 & 2.4665 & 1.1787 & 2.0066 & {\ul{\textbf{14.9681}}} \\ \hline
        GRU & 12.1855 & 3.6176 & 3.881 & 3.5758 & 5.0462 & {\ul{\textbf{28.3061}}} \\ \hline
        RNN & 11.2289 & 3.6152 & 6.8748 & 3.0483 & 4.8869 & {\ul{\textbf{29.6541}}} \\ \hline
        LSTM & 15.5649 & 3.6178 & 4.2537 & 3.6141 & 5.1415 & {\ul{\textbf{32.1920}}} \\ \hline
        LinReg & 13.465 & 3.661 & 7.8551 & 8.4248 & 9.5464 & {\ul{\textbf{42.9523}}} \\ \hline
        ARIMA & 14.9971 & 4.1595 & 7.4077 & 8.4354 & 9.5993 & {\ul{\textbf{44.5990}}} \\ \hline
        MLP & 15.3179 & 3.7288 & 7.5508 & 8.6366 & 10.6112 & {\ul{\textbf{45.8453}}} \\ \hline
    \end{tabular}
\end{table*}

In this section, we start by presenting the datasets used to conduct the experiments.
Afterwards, we list the baselines used in the experiments and describe general challenges involved in time series forecasting.
Finally, we present the results from the experiments and analyse them.

- Introduction 
- Present the results and the table
- Say that we have proved that the transformer is the best model for time series forecasting (out of the models we have tested, given our datasets and the hyperparameters we used within the computational limitations imposed on us.)
    - Explain why it is better 
    - 

    
\subsection{Datasets}
The following datasets have been used when conducting experiments on the various machine learning algorithms used in this paper.
\begin{itemize}
    \item \textbf{Austin:} The raw dataset over weather features in Austin, Texas covers a 4 year period from 2013 to 2017 with data logged once per day in the period.
    \item \textbf{Bangladesh:} The weather dataset for Bangladesh was collected during the period 1901 to 2015 and shows the average monthly temperature during this period. 
    \item \textbf{Delhi:} The dataset covers weather data for Delhi, India from 2013 to 2017 with one day between each logged data point.
    \item \textbf{Szeged:} The raw dataset covers weather data for Szeged, Hungary from 2006 to 2016 generated every hour over a 10-year span.
    \item \textbf{Jena:} The dataset was recorded at the Max Planck Institute's weather station in Jena, Germany from 2009 to 2016.
\end{itemize}

\begin{table*}[!ht]
\centering
\caption {Dataset Statistics} \label{tab:DatasetTable}
\begin{tabular}{|l|r|r|ll}
\cline{1-3}
Dataset    & \multicolumn{1}{l|}{\#Samples} & \multicolumn{1}{l|}{\#Sample rate} &  &  \\ \cline{1-3}
Austin     & 1319                           & 1 Day                              &  &  \\ \cline{1-3}
Bangladesh & 1380                           & 1 Month                            &  &  \\ \cline{1-3}
Delhi      & 1578                           & 1 Day                              &  &  \\ \cline{1-3}
Jena       & 420451                         & 10 Minutes                         &  &  \\ \cline{1-3}
Szeged     & 96453                          & 1 Hour                             &  &  \\ \cline{1-3}
\end{tabular}
\end{table*}

We have used the root-mean-square-error (RMSE) as the loss function to compare the chosen machine learning algorithms. The formula for computing the RMSE can be seen below: 
$$RMSE = \sqrt{\frac 1 n \displaystyle\sum_{i=1}^n(Y_i - \hat{Y_i})^2}$$

\subsection{Baselines}

These models are taken from statistical, machine learning, and deep learning to get a better overview of how the transformer's results compare with other well-known models.
\begin{itemize}
    \item ARIMA\cite{ARIMA-book}: A generalisation of the ARMA model that can be fitted to data in order to predict future steps.
    \item GRU\cite{GRU-paper}: A RNN model similar to LSTM but with a forget-gate added instead of an output gate.
    \item LinReg: A standard linear regression model.
    \item LSTM\cite{LSTM-paper}: A model modifying the RNN memory system in order to combat the common vanishing gradient problem.
    \item MLP\cite{MLP-paper}: The classic neural network used for regression prediction problems.
    \item RNN\cite{RNN-paper}: The standard recurrent neural network model.
    \item Transformer\cite{AttentionIsAllYouNeed}: A state of art model used in NLP, modified to work for time series forecasting.
\end{itemize}

\subsection{Results}
Table \ref{tab:ResultsTable} summarizes the best performances of the tested models for each of the five datasets used. 
As can be seen, the transformer performs the best in all datasets. The reason for this is two-fold. 
The first reason is the time embedding component in the transformer, which allows the transformer to account for both periodic and non-periodic data and thus account for seasonality in the data. 
The second reason is the multi-head attention mechanism which, combined with the time embedding, allows the transformer to account for long-term dependencies. Models such as ARIMA can account for seasonality but struggles with long term dependencies. 

RNNs are able to account for seasonality, but as described in xx they are expected to falter with long time steps due to the vanishing and exploding gradient problem. Table \ref{tab:ResultsTable} does, however, show that RNN performs well on all datasets except for Delhi dataset. This issue may be related to the structure of the data or tuning of the hyperparameters, but analysis of this issue is out of scope for the paper.
 
LSTMs and GRUs are also able to account for seasonality, but are expected to perform better with long time steps than RNNs. There are, however, outliers as the two models in our experiment are being outperformed on all datasets except for the Delhi dataset. This relates to the previously mentioned issue.

Linear regression and MLP are behaving as expected.