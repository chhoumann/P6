\begin{table*}[t]
    \centering
    \caption {Summary of gathered results---the best results are highlighted in bold and underlined.} \label{tab:ResultsTable} 
    \begin{tabular}{|l|r|r|r|r|r|}
    \hline
    \multicolumn{1}{|c|}{Models} & \multicolumn{1}{c|}{Austin} & \multicolumn{1}{c|}{Bangladesh} & \multicolumn{1}{c|}{Delhi} & \multicolumn{1}{c|}{Jena} & \multicolumn{1}{l|}{Szeged} \\ \hline
    ARIMA                        & xxxxxxx                     & xxxxxx                          & 7.2920                     & xxxxx                     & xxxxx                       \\ \hline
    GRU                          & 12.1855                     & 13.0872                         & 3.8810                     & 3.5758                    & 5.0462                      \\ \hline
    LinReg                       & 13.465                      & 3.6610                          & 7.8551                     & 8.4248                    & 9.5464                      \\ \hline
    LSTM                         & 15.5649                     & 3.6178                          & 4.2537                     & 3.6141                    & 5.1415                      \\ \hline
    MLP                          & 15.3179                     & 3.7288                          & 7.5508                     & 8.6366                    & 10.6112                     \\ \hline
    RNN                          & 11.2289                     & 3.6152                          & 6.8748                     & 3.0483                    & 4.8869                      \\ \hline
    Transformer                  & 7.4627                      & 1.8536                          & 2.4665                     & 1.1787                    & xxxxx                       \\ \hline
    \end{tabular}
\end{table*}

\subsection{Datasets}
The following datasets have been used when conducting experiments on the various machine learning algorithms used in this paper.
\begin{itemize}
    \item \textbf{Austin:} The raw dataset over weather features in Austin, Texas covers a 4 year period from 2013 to 2017 with data logged once per day in the period.
    \item \textbf{Bangladesh:} The weather dataset for Bangladesh was collected during the period 1901 to 2015 and shows the average monthly temperature during this period. 
    \item \textbf{Delhi:} The dataset covers weather data for Delhi, India from 2013 to 2017 with one day between each logged data point.
    \item \textbf{Szeged:} The raw dataset covers weather data for Szeged, Hungary from 2006 to 2016 generated every hour over a 10-year span.
    \item \textbf{Jena:} The dataset was recorded at the Max Planck Institute's weather station in Jena, Germany from 2009 to 2016.
\end{itemize}

\begin{table*}[!ht]
\centering
\caption {Dataset Statistics} \label{tab:DatasetTable}
\begin{tabular}{|l|r|r|ll}
\cline{1-3}
Dataset    & \multicolumn{1}{l|}{\#Samples} & \multicolumn{1}{l|}{\#Sample rate} &  &  \\ \cline{1-3}
Austin     & 1319                           & 1 Day                              &  &  \\ \cline{1-3}
Bangladesh & 1380                           & 1 Month                            &  &  \\ \cline{1-3}
Delhi      & 100990                         & 3 Hours                            &  &  \\ \cline{1-3}
Jena       & 420451                         & 10 Minutes                         &  &  \\ \cline{1-3}
Szeged     & 96453                          & 1 Hour                             &  &  \\ \cline{1-3}
\end{tabular}
\end{table*}

This paper uses root-mean-square-error (RMSE) as the loss function to compare the chosen machine learning algorithms. The formula for computing the RMSE can be seen below: 
$$RMSE = \sqrt{\frac 1 n \displaystyle\sum_{i=1}^n(Y_i - \hat{Y_i})^2}$$

\subsection{Benchmarks}
Below is a comparison of the best results computed from the following models. These models are taken from statistical, machine learning, and deep learning to get a better overview of how the transformer's results compare with other well-known models.
\begin{itemize}
    \item ARIMA\cite{ARIMA-book}: A generalisation of the ARMA model that can be fitted to data in order to predict future steps.
    \item GRU\cite{GRU-paper}: A RNN model similar to LSTM but with a forget-gate added instead of an output gate.
    \item LinReg: A standard linear regression model.
    \item LSTM\cite{LSTM-paper}: A model modifying the RNN memory system in order to combat the common vanishing gradient problem.
    \item MLP\cite{MLP-paper}: The classic neural network used for regression prediction problems.
    \item RNN\cite{RNN-paper}: The standard recurrent neural network model.
    \item Transformer\cite{AttentionIsAllYouNeed}: A state of art model used in NLP, modified to work for time series forecasting.
\end{itemize}

\subsection{Implementation Details}
All experiments included in this report have been conducted on Deepnote workspaces\cite{deepnote}.

\subsection{Results}
Table \ref{tab:ResultsTable} summarizes the best performances of the tested models for each of the five datasets used. 
Note how the transformer model is the best performing model for 4/5 of the used datasets.
