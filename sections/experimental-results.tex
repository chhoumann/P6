\begin{table*}[t]
    \centering
    \caption {Summary of gathered results---the best results are highlighted in bold and underlined.} \label{tab:ResultsTable} 
    \begin{tabular}{|l|r|r|r|r|r|}
    \hline
    \multicolumn{1}{|c|}{Models} & \multicolumn{1}{c|}{Austin} & \multicolumn{1}{c|}{Bangladesh} & \multicolumn{1}{c|}{Delhi} & \multicolumn{1}{c|}{Jena} & \multicolumn{1}{c|}{Szeged} \\ \hline
    ARIMA                        & 14.9971                     & 4.1595                          & 7.4077                     & 8.4354                    & 9.5993                      \\ \hline
    GRU                          & 12.1855                     & 3.6176                          & 3.8810                     & 3.5758                    & 5.0462                      \\ \hline
    LinReg                       & 13.465                      & 3.6610                          & 7.8551                     & 8.4248                    & 9.5464                      \\ \hline
    LSTM                         & 15.5649                     & 3.6178                          & 4.2537                     & 3.6141                    & 5.1415                      \\ \hline
    MLP                          & 15.3179                     & 3.7288                          & 7.5508                     & 8.6366                    & 10.6112                     \\ \hline
    RNN                          & 11.2289                     & 3.6152                          & 6.8748                     & 3.0483                    & 4.8869                      \\ \hline
    Transformer                  & {\ul{\textbf{7.4627}}}      & {\ul{\textbf{1.8536}}}          & {\ul{\textbf{2.4665}}}     & {\ul{\textbf{1.1787}}}    & {\ul{\textbf{2.0066}}}      \\ \hline
    \end{tabular}
\end{table*}

\begin{table*}[t]
    \centering
    \caption {Results normalized using standard deviation and sorted by lowest summed error in ascending order for each dataset.} \label{tab:ResultsTableSummed} 
    \begin{tabular}{|l|r|r|r|r|r|r|}
    \hline
        \multicolumn{1}{|c|}{Models} & \multicolumn{1}{c|}{Austin} & \multicolumn{1}{c|}{Bangladesh} & \multicolumn{1}{c|}{Delhi} & \multicolumn{1}{c|}{Jena} & \multicolumn{1}{c|}{Szeged} & \multicolumn{1}{c|}{Sum} \\ \hline
        Transformer & 0.4872 & 0.4971 & 0.3267 & 0.1365 & 0.1890 & {\ul{\textbf{1.6365}}} \\ \hline
        GRU & 0.7955 & 0.9702 & 0.5140 & 0.4140 & 0.4756 & {\ul{\textbf{3.1693}}} \\ \hline
        RNN & 0.7331 & 0.9695 & 0.9105 & 0.3530 & 0.4605 & {\ul{\textbf{3.4266}}} \\ \hline
        LSTM & 1.0161 & 0.9702 & 0.5633 & 0.4185 & 0.4845 & {\ul{\textbf{3.4527}}} \\ \hline
        LinReg & 0.8790 & 0.9818 & 1.0403 & 0.9755 & 0.8997 & {\ul{\textbf{4.7763}}} \\ \hline
        ARIMA & 0.9791 & 1.1155 & 0.9810 & 0.9767 & 0.9046 & {\ul{\textbf{4.9570}}} \\ \hline
        MLP & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & {\ul{\textbf{5.0000}}} \\ \hline
    \end{tabular}
\end{table*}

In this section, we start by presenting the datasets used to conduct the experiments.
Afterwards, we list the baselines used in the experiments and describe general challenges involved in time series forecasting.
Finally, we present the results from the experiments and analyze them.

\subsection{Datasets}
The following datasets have been used when conducting experiments on the various models used in this paper.
\begin{itemize}
    \item \textbf{Austin}\cite{HistoricalHourlyWeather}: The raw dataset over weather features in Austin, Texas covers a four-year period from 2013 to 2017 with data logged once per day in the period.
    \item \textbf{Bangladesh}\cite{BangladeshWeatherDataset}: The weather dataset for Bangladesh was collected during the period 1901 to 2015 and shows the average monthly temperature during this period. 
    \item \textbf{Delhi}\cite{DailyClimateTime}: The dataset covers weather data for Delhi, India from 2013 to 2017 with one day between each logged data point.
    \item \textbf{Szeged}\cite{WeatherSzeged}: The raw dataset covers weather data for Szeged, Hungary from 2006 to 2016 generated every hour over a 10-year span.
    \item \textbf{Jena}\cite{WeatherArchiveJena}: The dataset was recorded at the Max Planck Institute's weather station in Jena, Germany from 2009 to 2016.
\end{itemize}

\begin{table*}[!ht]
\centering
\caption {Dataset Statistics} \label{tab:DatasetTable}
\begin{tabular}{|l|r|r|ll}
\cline{1-3}
Dataset    & \multicolumn{1}{l|}{\#Samples} & \multicolumn{1}{l|}{\#Sample rate} &  &  \\ \cline{1-3}
Austin     & 1319                           & 1 Day                              &  &  \\ \cline{1-3}
Bangladesh & 1380                           & 1 Month                            &  &  \\ \cline{1-3}
Delhi      & 1578                           & 1 Day                              &  &  \\ \cline{1-3}
Jena       & 420451                         & 10 Minutes                         &  &  \\ \cline{1-3}
Szeged     & 96453                          & 1 Hour                             &  &  \\ \cline{1-3}
\end{tabular}
\end{table*}

We have used the root-mean-square-error (RMSE) as the loss function to compare the chosen models.
The formula for computing the RMSE can be seen below: 
$$RMSE = \sqrt{\frac 1 n \displaystyle\sum_{i=1}^n(Y_i - \hat{Y_i})^2}$$

where $Y$ is the observed target feature values, and $\hat{Y}$ is the predicted target feature value.

\subsection{Baselines}

These models are taken from statistics and machine learning to get a better overview of how the transformer's results compare with other well-known models.
\begin{itemize}
    \item \textbf{ARIMA}\cite{ARIMA-book}: A generalization of the ARMA model that can be fitted to data in order to predict future steps.
    \item \textbf{GRU}\cite{GRU-paper}: An RNN model similar to LSTM but with a forget gate added instead of an output gate.
    \item \textbf{LinReg}: A standard linear regression model.
    \item \textbf{LSTM}\cite{LSTMPaper}: A model modifying the RNN memory system in order to combat its long term dependency issues.
    \item \textbf{MLP}\cite{MLP-paper}: The classic neural network used for regression prediction problems.
    \item \textbf{RNN}\cite{RNN-paper}: The standard recurrent neural network model.
\end{itemize}

\subsection{Results}\label{sec:results}
Table \ref{tab:ResultsTable} summarizes the best performances of the tested models for each of the five datasets used. 
As can be seen, the transformer performs the best in all datasets. 
There are several probable reasons for this. 
The time embedding component in the transformer allows it to account for both periodic and non-periodic data, and thus account for seasonality in the data. 
The multi-head attention mechanism which, combined with the time embedding, allows the transformer to account for long-term dependencies. Models such as ARIMA can account for seasonality but struggles with long-term dependencies. 

RNNs are able to account for seasonality, but as described in section \ref{subsec:RNNModels}, they are expected to falter over long time steps due to the vanishing and exploding gradient problems. Table \ref{tab:ResultsTable} does, however, show that RNN performs well on all datasets except for the Delhi dataset. This issue may be related to the structure or size of the data, or tuning of the hyperparameters, however, we will look into this behavior in our future work.

LSTMs and GRUs can also account for seasonality but are expected to perform better over long time steps than RNNs. 
However, the LSTM model in our experiment is outperformed by the RNN model on all datasets except for the Delhi dataset.
We expected LSTM to perform similarly to GRU and consequently, we expected it to outperform the RNN model.

Linear regression and MLP are behaving as expected.

Standard deviation normalization was performed to account for the variability in the temperature depending on the region of the world in which the temperature data was captured. 
For example, data captured close to the equator may be higher and therefore the RMSE is also amplified in equal proportion. 
In order to compute the values seen in table \ref{tab:ResultsTableSummed}, the model errors from table \ref{tab:ResultsTable} were divided by the standard deviation calculated for each dataset.
The sum column shows the overall performance for each model, where the lowest number represents the best-performing model. 